# LY-Major-Project
HandsOn Connect: A real-time sign language translation application using Machine Learning

## Scope
The goal of HandsOn Connect is to develop a user-friendly web application using the MERN stack, employing machine learning models to facilitate real-time interpretation of sign language gestures. The application aims to bridge communication barriers between individuals proficient in sign language and those who are not, catering to the deaf and hard-of-hearing community. The primary focus involves translating sign language gestures into English alphabets (A-Z), numeric digits (0-9), and a limited set of common English words, captured through a camera feed and converted into text.

## Codebase:

- Python scripts using TensorFlow/Keras for the deep learning model.
- Data preprocessing scripts using Pandas, NumPy, Matplotlib, and Seaborn.
- Data augmentation, splitting, and preparation scripts.

## DataSet: 
The project utilizes an American Sign Language (ASL) dataset for training and validating the sign language recognition model. Below are key details regarding the dataset:
### Dataset Name: 
American Sign Language (ASL) Dataset
### Source: 
You can access the original dataset on Kaggle [Dataset]([https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment](https://www.kaggle.com/datasets/ayuraj/asl-dataset/data)). 
### Description: 
The dataset contains images representing sign language gestures corresponding to English alphabets (a-z) and numeric digits (0-9).
