# LY-Major-Project
HandsOn Connect: A real-time sign language translation application using Machine Learning

## Scope
The goal of HandsOn Connect is to develop a user-friendly web application using the MERN stack, employing machine learning models to facilitate real-time interpretation of sign language gestures. The application aims to bridge communication barriers between individuals proficient in sign language and those who are not, catering to the deaf and hard-of-hearing community. The primary focus involves translating sign language gestures into English alphabets (A-Z), numeric digits (0-9), and a limited set of common English words, captured through a camera feed and converted into text.

## Codebase:

- Python scripts using TensorFlow/Keras for the deep learning model.
- Data preprocessing scripts using Pandas, NumPy, Matplotlib, and Seaborn.
- Data augmentation, splitting, and preparation scripts.

## Prerequisites

Before using this code, you should have the following installed:

- Python (>=3.7)
- Jupyter Notebook (optional, but recommended)

## Installation

1. Clone the repository:

   ```sh
   git clone https://github.com/mayankpujara/LY-Major-Project.git
2. Dataset and Resources
ASL Dataset:
- Download the ASL dataset used for training the model from ASL Dataset on Kaggle.
- Ensure the dataset is structured and stored according to the project's requirements.
Kaggle API (if downloading from Kaggle):
Obtain a Kaggle API key and configure it for downloading datasets using the Kaggle API.

## DataSet: 
The project utilizes an American Sign Language (ASL) dataset for training and validating the sign language recognition model. Below are key details regarding the dataset:
### Dataset Name: 
American Sign Language (ASL) Dataset
### Source: 
You can access the original dataset on Kaggle [Dataset](https://www.kaggle.com/datasets/ayuraj/asl-dataset/data). 
### Description: 
The dataset contains images representing sign language gestures corresponding to English alphabets (a-z) and numeric digits (0-9).
